{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vLPbAJ0h4FU"
   },
   "source": [
    "# Sentiment Analysis - Twitter, IMDB, YELP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23672,
     "status": "ok",
     "timestamp": 1683176832855,
     "user": {
      "displayName": "venkat suhruth kanakamedala",
      "userId": "16602201270286533116"
     },
     "user_tz": 300
    },
    "id": "B-MnY-1Dh81M",
    "outputId": "9b1c5759-06f7-4e97-9bea-d8cca07f9dca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19195,
     "status": "ok",
     "timestamp": 1683176867469,
     "user": {
      "displayName": "venkat suhruth kanakamedala",
      "userId": "16602201270286533116"
     },
     "user_tz": 300
    },
    "id": "HdiZqbpeh4FY",
    "outputId": "8e161faa-4270-4bc6-f3ff-e801f8f72084"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 378,
     "status": "ok",
     "timestamp": 1683176872916,
     "user": {
      "displayName": "venkat suhruth kanakamedala",
      "userId": "16602201270286533116"
     },
     "user_tz": 300
    },
    "id": "ZbKgNfk6h4Fa"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 5463,
     "status": "ok",
     "timestamp": 1683176880827,
     "user": {
      "displayName": "venkat suhruth kanakamedala",
      "userId": "16602201270286533116"
     },
     "user_tz": 300
    },
    "id": "0VNNonASh4Fa"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/content/drive/MyDrive/BSAN Projects/NLP_735_Team1/im_Train.csv', encoding='utf-8')\n",
    "test_df = pd.read_csv('/content/drive/MyDrive/BSAN Projects/NLP_735_Team1/im_Test.csv', encoding='utf-8')\n",
    "val_df = pd.read_csv('/content/drive/MyDrive/BSAN Projects/NLP_735_Team1/im_valid.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1683176883881,
     "user": {
      "displayName": "venkat suhruth kanakamedala",
      "userId": "16602201270286533116"
     },
     "user_tz": 300
    },
    "id": "CYDMeZ_ZbJC8",
    "outputId": "ee8ad8de-f611-4767-f081-df0b3838d50f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'label'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1683176887953,
     "user": {
      "displayName": "venkat suhruth kanakamedala",
      "userId": "16602201270286533116"
     },
     "user_tz": 300
    },
    "id": "7iG_QdCAh4Fc"
   },
   "outputs": [],
   "source": [
    "test = copy.deepcopy(test_df)\n",
    "train = copy.deepcopy(train_df)\n",
    "valid = copy.deepcopy(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCwSSudTh4Ff"
   },
   "source": [
    "# text pre process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3651,
     "status": "ok",
     "timestamp": 1683176894584,
     "user": {
      "displayName": "venkat suhruth kanakamedala",
      "userId": "16602201270286533116"
     },
     "user_tz": 300
    },
    "id": "gXqdy0iQh4Ff",
    "outputId": "0306bc1a-f123-4d76-b069-cbe83fcce0e8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9198,
     "status": "ok",
     "timestamp": 1683176906758,
     "user": {
      "displayName": "venkat suhruth kanakamedala",
      "userId": "16602201270286533116"
     },
     "user_tz": 300
    },
    "id": "sYAmR2eEh4Ff",
    "outputId": "a289e40a-4941-484d-c4ce-2da9196f5d69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting contractions\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting pyahocorasick\n",
      "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting anyascii\n",
      "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n"
     ]
    }
   ],
   "source": [
    "pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 376,
     "status": "ok",
     "timestamp": 1683176911093,
     "user": {
      "displayName": "venkat suhruth kanakamedala",
      "userId": "16602201270286533116"
     },
     "user_tz": 300
    },
    "id": "XxtoWdifh4Ff",
    "outputId": "6f7ee3d5-ef44-4a21-976a-0714aff67940"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample sentence not ignored\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "import unicodedata\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def text_preprocess(text):\n",
    "    # Convert input to string\n",
    "    text = str(text)\n",
    "    \n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # Remove punctuation and stress marks using Unicode\n",
    "    text = ''.join(c for c in text if not unicodedata.category(c).startswith('P'))\n",
    "    text = ''.join(c for c in text if not unicodedata.category(c).startswith('Sk'))\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Lemmatize words using NLTK\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Remove stopwords, excluding negation words\n",
    "    negation_words = {'not', 'no', 'n\\'t', 'never', 'none', 'nobody', 'nothing', 'neither', 'nowhere', 'cant', 'can\\'t', 'dont', 'don\\'t'}\n",
    "    stop_words = set(stopwords.words('english')) - negation_words\n",
    "    filtered_tokens = [token for token in lemmatized_tokens if isinstance(token, str) and token.lower() not in stop_words]\n",
    "\n",
    "    # Reconstruct the processed text\n",
    "    processed_text = ' '.join(filtered_tokens)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "# Example usage\n",
    "input_text = \"This is a sample sentence, which won't be ignored.\"\n",
    "output_text = text_preprocess(input_text)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 144431,
     "status": "ok",
     "timestamp": 1683177059420,
     "user": {
      "displayName": "venkat suhruth kanakamedala",
      "userId": "16602201270286533116"
     },
     "user_tz": 300
    },
    "id": "wnJ4K81qh4Fg"
   },
   "outputs": [],
   "source": [
    "train['preprocessed_text'] = train['text'].apply(text_preprocess) \n",
    "test['preprocessed_text'] = test['text'].apply(text_preprocess) \n",
    "valid['preprocessed_text'] = valid['text'].apply(text_preprocess) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 344,
     "status": "ok",
     "timestamp": 1683177115693,
     "user": {
      "displayName": "venkat suhruth kanakamedala",
      "userId": "16602201270286533116"
     },
     "user_tz": 300
    },
    "id": "KOkfCMSmeW51"
   },
   "outputs": [],
   "source": [
    "imdb = pd.concat([train,test,valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1683177117637,
     "user": {
      "displayName": "venkat suhruth kanakamedala",
      "userId": "16602201270286533116"
     },
     "user_tz": 300
    },
    "id": "5SV6sWwxh4Fg",
    "outputId": "62d7ff84-1b13-4452-b6f0-b2ed9e684255"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-27a98c10-f797-4f0a-b46e-0cb71b9fc300\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I grew up (b. 1965) watching and loving the Th...</td>\n",
       "      <td>0</td>\n",
       "      <td>grew b 1965 watching loving Thunderbirds mate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I put this movie in my DVD player, and sa...</td>\n",
       "      <td>0</td>\n",
       "      <td>put movie DVD player sat coke chip expectation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why do people who do not know what a particula...</td>\n",
       "      <td>0</td>\n",
       "      <td>people not know particular time past wa like f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even though I have great interest in Biblical ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Even though great interest Biblical movie wa b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im a die hard Dads Army fan and nothing will e...</td>\n",
       "      <td>1</td>\n",
       "      <td>die hard Dads Army fan nothing ever change got...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-27a98c10-f797-4f0a-b46e-0cb71b9fc300')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-27a98c10-f797-4f0a-b46e-0cb71b9fc300 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-27a98c10-f797-4f0a-b46e-0cb71b9fc300');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  I grew up (b. 1965) watching and loving the Th...      0   \n",
       "1  When I put this movie in my DVD player, and sa...      0   \n",
       "2  Why do people who do not know what a particula...      0   \n",
       "3  Even though I have great interest in Biblical ...      0   \n",
       "4  Im a die hard Dads Army fan and nothing will e...      1   \n",
       "\n",
       "                                   preprocessed_text  \n",
       "0  grew b 1965 watching loving Thunderbirds mate ...  \n",
       "1  put movie DVD player sat coke chip expectation...  \n",
       "2  people not know particular time past wa like f...  \n",
       "3  Even though great interest Biblical movie wa b...  \n",
       "4  die hard Dads Army fan nothing ever change got...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdYta-iIh4Fg"
   },
   "source": [
    "# Nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 207477,
     "status": "ok",
     "timestamp": 1683177329886,
     "user": {
      "displayName": "venkat suhruth kanakamedala",
      "userId": "16602201270286533116"
     },
     "user_tz": 300
    },
    "id": "i0lro7PDh4Fg",
    "outputId": "559dcb05-e2fe-45df-9b61-e6fd6f8ea464"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer fold 5\n",
      "Inner fold 10\n",
      "Inner fold 10, C=10, F1 Score: 0.9027058965798582\n",
      "Inner fold 11\n",
      "Inner fold 11, C=10, F1 Score: 0.8904213056902488\n",
      "Inner fold 12\n",
      "Inner fold 12, C=10, F1 Score: 0.8935690631981597\n",
      "Inner fold 13\n",
      "Inner fold 13, C=10, F1 Score: 0.8934257526484781\n",
      "Inner fold 14\n",
      "Inner fold 14, C=10, F1 Score: 0.8979919975762998\n",
      "Best inner params for outer fold 5: {'clf__C': 10}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.88      0.90      1500\n",
      "           1       0.88      0.92      0.90      1500\n",
      "\n",
      "    accuracy                           0.90      3000\n",
      "   macro avg       0.90      0.90      0.90      3000\n",
      "weighted avg       0.90      0.90      0.90      3000\n",
      "\n",
      "Outer fold 6\n",
      "Inner fold 10\n",
      "Inner fold 10, C=10, F1 Score: 0.9027058965798582\n",
      "Inner fold 11\n",
      "Inner fold 11, C=10, F1 Score: 0.8904213056902488\n",
      "Inner fold 12\n",
      "Inner fold 12, C=10, F1 Score: 0.8935690631981597\n",
      "Inner fold 13\n",
      "Inner fold 13, C=10, F1 Score: 0.8934257526484781\n",
      "Inner fold 14\n",
      "Inner fold 14, C=10, F1 Score: 0.8979919975762998\n",
      "Best inner params for outer fold 6: {'clf__C': 10}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.90      1500\n",
      "           1       0.90      0.91      0.90      1500\n",
      "\n",
      "    accuracy                           0.90      3000\n",
      "   macro avg       0.90      0.90      0.90      3000\n",
      "weighted avg       0.90      0.90      0.90      3000\n",
      "\n",
      "Outer fold 7\n",
      "Inner fold 10\n",
      "Inner fold 10, C=10, F1 Score: 0.9027058965798582\n",
      "Inner fold 11\n",
      "Inner fold 11, C=10, F1 Score: 0.8904213056902488\n",
      "Inner fold 12\n",
      "Inner fold 12, C=10, F1 Score: 0.8935690631981597\n",
      "Inner fold 13\n",
      "Inner fold 13, C=10, F1 Score: 0.8934257526484781\n",
      "Inner fold 14\n",
      "Inner fold 14, C=10, F1 Score: 0.8979919975762998\n",
      "Best inner params for outer fold 7: {'clf__C': 10}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.89      0.90      1500\n",
      "           1       0.89      0.92      0.90      1500\n",
      "\n",
      "    accuracy                           0.90      3000\n",
      "   macro avg       0.90      0.90      0.90      3000\n",
      "weighted avg       0.90      0.90      0.90      3000\n",
      "\n",
      "Outer fold 8\n",
      "Inner fold 10\n",
      "Inner fold 10, C=10, F1 Score: 0.9027058965798582\n",
      "Inner fold 11\n",
      "Inner fold 11, C=10, F1 Score: 0.8904213056902488\n",
      "Inner fold 12\n",
      "Inner fold 12, C=10, F1 Score: 0.8935690631981597\n",
      "Inner fold 13\n",
      "Inner fold 13, C=10, F1 Score: 0.8934257526484781\n",
      "Inner fold 14\n",
      "Inner fold 14, C=10, F1 Score: 0.8979919975762998\n",
      "Best inner params for outer fold 8: {'clf__C': 10}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      1500\n",
      "           1       0.89      0.89      0.89      1500\n",
      "\n",
      "    accuracy                           0.89      3000\n",
      "   macro avg       0.89      0.89      0.89      3000\n",
      "weighted avg       0.89      0.89      0.89      3000\n",
      "\n",
      "Outer fold 9\n",
      "Inner fold 10\n",
      "Inner fold 10, C=10, F1 Score: 0.9027058965798582\n",
      "Inner fold 11\n",
      "Inner fold 11, C=10, F1 Score: 0.8904213056902488\n",
      "Inner fold 12\n",
      "Inner fold 12, C=10, F1 Score: 0.8935690631981597\n",
      "Inner fold 13\n",
      "Inner fold 13, C=10, F1 Score: 0.8934257526484781\n",
      "Inner fold 14\n",
      "Inner fold 14, C=10, F1 Score: 0.8979919975762998\n",
      "Best inner params for outer fold 9: {'clf__C': 10}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.90      1500\n",
      "           1       0.90      0.91      0.90      1500\n",
      "\n",
      "    accuracy                           0.90      3000\n",
      "   macro avg       0.90      0.90      0.90      3000\n",
      "weighted avg       0.90      0.90      0.90      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "def train_and_evaluate(X_train_all, y_train_all, X_test_all, y_test_all):\n",
    "    skf_outer = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    skf_inner = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "    outer_fold = 5\n",
    "\n",
    "    for train_index, test_index in skf_outer.split(X_test_all, y_test_all):\n",
    "        print('Outer fold', outer_fold)\n",
    "\n",
    "        X_test, y_test = np.array(X_test_all)[test_index], np.array(y_test_all)[test_index]\n",
    "\n",
    "        inner_fold = 10\n",
    "        best_inner_score = 0\n",
    "        best_inner_params = None\n",
    "\n",
    "        for sub_train_index, dev_index in skf_inner.split(X_train_all, y_train_all):\n",
    "            print('Inner fold', inner_fold)\n",
    "\n",
    "            X_train, X_dev = np.array(X_train_all)[sub_train_index], np.array(X_train_all)[dev_index]\n",
    "            y_train, y_dev = np.array(y_train_all)[sub_train_index], np.array(y_train_all)[dev_index]\n",
    "\n",
    "            clf = Pipeline([('vect', TfidfVectorizer(smooth_idf=True)),\n",
    "                            ('clf', LogisticRegression(penalty='l2', solver='liblinear', max_iter=1500, random_state=0))])\n",
    "\n",
    "            for C in [10]:  # Tune the 'C' hyperparameter\n",
    "                clf.set_params(clf__C=C)\n",
    "                clf.fit(X_train, y_train)\n",
    "                y_dev_pred = clf.predict(X_dev)\n",
    "\n",
    "                dev_f1_score = f1_score(y_dev, y_dev_pred, average='macro')\n",
    "                print(f\"Inner fold {inner_fold}, C={C}, F1 Score: {dev_f1_score}\")\n",
    "\n",
    "                if dev_f1_score > best_inner_score:\n",
    "                    best_inner_score = dev_f1_score\n",
    "                    best_inner_params = {'clf__C': C}\n",
    "\n",
    "            inner_fold += 1\n",
    "\n",
    "        print(f\"Best inner params for outer fold {outer_fold}: {best_inner_params}\")\n",
    "\n",
    "        # Train the model using the best inner params on the entire train set of the current outer fold\n",
    "        clf.set_params(**best_inner_params)\n",
    "        clf.fit(X_train_all, y_train_all)\n",
    "\n",
    "        y_test_pred = clf.predict(X_test)\n",
    "        print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "        outer_fold += 1\n",
    "\n",
    "# Load your dataset here\n",
    "# For example, let's assume you have a dataset called `tweets` with preprocessed_text and sentiment columns\n",
    "X = imdb['preprocessed_text']\n",
    "y = imdb['label']\n",
    "\n",
    "# Split the dataset into train and test sets using train_test_split\n",
    "X_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Call the train_and_evaluate function with the input data\n",
    "train_and_evaluate(X_train_all, y_train_all, X_test_all, y_test_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df25_51Ih4Fh"
   },
   "source": [
    "# Trainable embed and Pre-trained GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 150868,
     "status": "ok",
     "timestamp": 1683177529795,
     "user": {
      "displayName": "venkat suhruth kanakamedala",
      "userId": "16602201270286533116"
     },
     "user_tz": 300
    },
    "id": "QC_XkEpFwOTH",
    "outputId": "389cd46c-9b2d-4273-a166-af1a198a8b28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 100)          16119000  \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               117248    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 258       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 2)                8         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,236,514\n",
      "Trainable params: 16,236,510\n",
      "Non-trainable params: 4\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 75s 136ms/step - loss: 0.3512 - accuracy: 0.8477 - val_loss: 0.2981 - val_accuracy: 0.8802\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 26s 53ms/step - loss: 0.1460 - accuracy: 0.9559 - val_loss: 0.3233 - val_accuracy: 0.8597\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 16s 33ms/step - loss: 0.0707 - accuracy: 0.9825 - val_loss: 0.4243 - val_accuracy: 0.8472\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 14s 29ms/step - loss: 0.0409 - accuracy: 0.9915 - val_loss: 0.3944 - val_accuracy: 0.8626\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 0.3890 - accuracy: 0.8636\n",
      "Test loss: 0.38901713490486145, Test accuracy: 0.8636000156402588\n",
      "313/313 [==============================] - 1s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.89      0.87      4951\n",
      "           1       0.88      0.84      0.86      5049\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, BatchNormalization, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming you have a dataset called imdb with 'preprocessed_text' and 'label' columns\n",
    "X = imdb['preprocessed_text']\n",
    "y = imdb['label']\n",
    "\n",
    "# Initialize the LabelBinarizer and fit it on the labels\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y)\n",
    "\n",
    "# Transform the labels into one-hot encoded format\n",
    "y_one_hot = lb.transform(y)\n",
    "\n",
    "# Tokenize the text data\n",
    "max_sequence_length = 100\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert y_train and y_test to one-hot encoding\n",
    "num_classes = 2  # Replace this with the number of unique labels in your dataset\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "# Update vocab_size for the model\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Create the Keras model with the same architecture you provided earlier\n",
    "embedding_dim = 100\n",
    "lstm_units = 128\n",
    "\n",
    "model_trainable = Sequential()\n",
    "model_trainable.add(Embedding(vocab_size, embedding_dim, input_length=max_sequence_length))\n",
    "model_trainable.add(LSTM(lstm_units))\n",
    "model_trainable.add(Dense(2))\n",
    "model_trainable.add(BatchNormalization())\n",
    "model_trainable.add(Activation('sigmoid'))\n",
    "\n",
    "# Compile the model with the specified optimizer settings\n",
    "initial_learning_rate = 0.001\n",
    "decay_steps = 1000\n",
    "decay_rate = 0.9\n",
    "\n",
    "lr_schedule = ExponentialDecay(initial_learning_rate, decay_steps, decay_rate)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "model_trainable.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model_trainable.summary()\n",
    "\n",
    "# Early stopping based on validation accuracy\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3)\n",
    "\n",
    "# Train the model\n",
    "history_trainable = model_trainable.fit(X_train, y_train_one_hot, epochs=10, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "test_loss, test_accuracy = model_trainable.evaluate(X_test, y_test_one_hot, batch_size=64)\n",
    "print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy}')\n",
    "\n",
    "# Get predicted probabilities for the test set\n",
    "y_pred_probs = model_trainable.predict(X_test)\n",
    "\n",
    "# Convert probabilities to predicted classes\n",
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Convert one-hot encoded y_test back to class labels\n",
    "y_test_classes = np.argmax(y_test_one_hot, axis=1)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_classes, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0p0Uq4Wh4Fi"
   },
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnhKhn8Ml7lc"
   },
   "source": [
    "# Load GloVe embeddings and create embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 109889,
     "status": "ok",
     "timestamp": 1683178181668,
     "user": {
      "displayName": "venkat suhruth kanakamedala",
      "userId": "16602201270286533116"
     },
     "user_tz": 300
    },
    "id": "EzBs9bJyOoeW",
    "outputId": "3ab50d0b-b6f2-4188-d06a-6bf1077c6134"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 100, 100)          16119000  \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               117248    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 2)                8         \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,236,514\n",
      "Trainable params: 117,510\n",
      "Non-trainable params: 16,119,004\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 12s 17ms/step - loss: 0.4585 - accuracy: 0.7816 - val_loss: 0.6210 - val_accuracy: 0.6884\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.3673 - accuracy: 0.8401 - val_loss: 0.3703 - val_accuracy: 0.8357\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.3233 - accuracy: 0.8606 - val_loss: 0.4109 - val_accuracy: 0.8338\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.2948 - accuracy: 0.8745 - val_loss: 0.3376 - val_accuracy: 0.8561\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 4s 9ms/step - loss: 0.2722 - accuracy: 0.8845 - val_loss: 0.3255 - val_accuracy: 0.8605\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 5s 9ms/step - loss: 0.2497 - accuracy: 0.8956 - val_loss: 0.3068 - val_accuracy: 0.8725\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.2257 - accuracy: 0.9090 - val_loss: 0.3289 - val_accuracy: 0.8608\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 4s 9ms/step - loss: 0.1994 - accuracy: 0.9200 - val_loss: 0.3244 - val_accuracy: 0.8669\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 5s 10ms/step - loss: 0.1730 - accuracy: 0.9329 - val_loss: 0.3707 - val_accuracy: 0.8581\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 0.3416 - accuracy: 0.8701\n",
      "Test loss: 0.34158068895339966, Test accuracy: 0.8701000213623047\n",
      "313/313 [==============================] - 2s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.81      0.86      4951\n",
      "           1       0.83      0.93      0.88      5049\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.88      0.87      0.87     10000\n",
      "weighted avg       0.88      0.87      0.87     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, BatchNormalization, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Assuming you have a dataset called imdb with 'preprocessed_text' and 'label' columns\n",
    "X = imdb['preprocessed_text']\n",
    "y = imdb['label']\n",
    "\n",
    "# Initialize the LabelBinarizer and fit it on the labels\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y)\n",
    "\n",
    "# Transform the labels into one-hot encoded format\n",
    "y_one_hot = lb.transform(y)\n",
    "\n",
    "# Tokenize the text data\n",
    "max_sequence_length = 100\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert y_train and y_test to one-hot encoding\n",
    "num_classes = 2  # Replace this with the number of unique labels in your dataset\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "# Update vocab_size for the model\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove_embeddings = {}\n",
    "with open('/content/drive/MyDrive/BSAN Projects/glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.strip().split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        glove_embeddings[word] = vector\n",
    "\n",
    "# Create GloVe embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = glove_embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "\n",
    "# Create the Keras model with the same architecture you provided earlier\n",
    "embedding_dim = 100\n",
    "lstm_units = 128\n",
    "\n",
    "model_trainable = Sequential()\n",
    "model_trainable.add(Embedding(vocab_size, embedding_dim, input_length=max_sequence_length, weights=[embedding_matrix], trainable=False))\n",
    "model_trainable.add(LSTM(lstm_units))\n",
    "model_trainable.add(Dense(2))\n",
    "model_trainable.add(BatchNormalization())\n",
    "model_trainable.add(Activation('sigmoid'))\n",
    "\n",
    "# Compile the model with the specified optimizer settings\n",
    "initial_learning_rate = 0.001\n",
    "decay_steps = 1000\n",
    "decay_rate = 0.9\n",
    "\n",
    "lr_schedule = ExponentialDecay(initial_learning_rate, decay_steps, decay_rate)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "model_trainable.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model_trainable.summary()\n",
    "\n",
    "# Early stopping based on validation accuracy\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3)\n",
    "\n",
    "# Train the model\n",
    "history_trainable = model_trainable.fit(X_train, y_train_one_hot, epochs=10, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model_trainable.evaluate(X_test, y_test_one_hot, batch_size=64)\n",
    "print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy}')\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get predicted probabilities for the test set\n",
    "y_pred_probs = model_trainable.predict(X_test)\n",
    "\n",
    "# Convert probabilities to predicted classes\n",
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Convert one-hot encoded y_test back to class labels\n",
    "y_test_classes = np.argmax(y_test_one_hot, axis=1)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_classes, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jhch78uJh4Fj"
   },
   "source": [
    "# Trainable embed and Avg. Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 159235,
     "status": "ok",
     "timestamp": 1683178474536,
     "user": {
      "displayName": "venkat suhruth kanakamedala",
      "userId": "16602201270286533116"
     },
     "user_tz": 300
    },
    "id": "h2cg6OHNPSrs",
    "outputId": "bb98bc3f-b5ca-48cf-fe7e-5153e8f3fb10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 100, 100)          16119000  \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 100, 128)          117248    \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 128)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 2)                8         \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,236,514\n",
      "Trainable params: 16,236,510\n",
      "Non-trainable params: 4\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "500/500 [==============================] - 57s 107ms/step - loss: 0.3548 - accuracy: 0.8490 - val_loss: 0.3583 - val_accuracy: 0.8489\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 20s 40ms/step - loss: 0.1750 - accuracy: 0.9474 - val_loss: 0.8970 - val_accuracy: 0.6394\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 13s 25ms/step - loss: 0.0989 - accuracy: 0.9778 - val_loss: 0.4579 - val_accuracy: 0.8084\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 12s 23ms/step - loss: 0.0574 - accuracy: 0.9906 - val_loss: 0.3601 - val_accuracy: 0.8482\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 0.3508 - accuracy: 0.8565\n",
      "Test loss: 0.3507883548736572, Test accuracy: 0.8565000295639038\n",
      "313/313 [==============================] - 2s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.81      0.85      4951\n",
      "           1       0.83      0.91      0.86      5049\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, BatchNormalization, Activation, GlobalAveragePooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming you have a dataset called imdb with 'preprocessed_text' and 'label' columns\n",
    "X = imdb['preprocessed_text']\n",
    "y = imdb['label']\n",
    "\n",
    "# Initialize the LabelBinarizer and fit it on the labels\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y)\n",
    "\n",
    "# Transform the labels into one-hot encoded format\n",
    "y_one_hot = lb.transform(y)\n",
    "\n",
    "# Tokenize the text data\n",
    "max_sequence_length = 100\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert y_train and y_test to one-hot encoding\n",
    "num_classes = 2  # Replace this with the number of unique labels in your dataset\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "# Update vocab_size for the model\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Create the Keras model with the same architecture you provided earlier\n",
    "embedding_dim = 100\n",
    "lstm_units = 128\n",
    "\n",
    "model_trainable = Sequential()\n",
    "model_trainable.add(Embedding(vocab_size, embedding_dim, input_length=max_sequence_length))\n",
    "model_trainable.add(LSTM(lstm_units, return_sequences=True))\n",
    "model_trainable.add(GlobalAveragePooling1D())\n",
    "model_trainable.add(Dense(2))\n",
    "model_trainable.add(BatchNormalization())\n",
    "model_trainable.add(Activation('sigmoid'))\n",
    "\n",
    "# Compile the model with the specified optimizer settings\n",
    "initial_learning_rate = 0.001\n",
    "decay_steps = 1000\n",
    "decay_rate = 0.9\n",
    "\n",
    "lr_schedule = ExponentialDecay(initial_learning_rate, decay_steps, decay_rate)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "model_trainable.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model_trainable.summary()\n",
    "\n",
    "# Early stopping based on validation accuracy\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3)\n",
    "\n",
    "# Train the model\n",
    "history_trainable = model_trainable.fit(X_train, y_train_one_hot, epochs=10, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "test_loss, test_accuracy = model_trainable.evaluate(X_test, y_test_one_hot, batch_size=64)\n",
    "print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy}')\n",
    "\n",
    "# Get predicted probabilities for the test set\n",
    "y_pred_probs = model_trainable.predict(X_test)\n",
    "\n",
    "# Convert probabilities to predicted classes\n",
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Convert one-hot encoded y_test back to class labels\n",
    "y_test_classes = np.argmax(y_test_one_hot, axis=1)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_classes, y_pred_classes))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1nxF43TiInLiyrc_cCguoedyX-XJejX9g",
     "timestamp": 1683130441059
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
