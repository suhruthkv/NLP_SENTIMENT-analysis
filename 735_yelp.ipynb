{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vLPbAJ0h4FU"
   },
   "source": [
    "# Sentiment Analysis - Twitter, IMDB, YELP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B-MnY-1Dh81M",
    "outputId": "d0d8acb1-3082-452c-968e-908919446e43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HdiZqbpeh4FY",
    "outputId": "2fdb48a2-6f29-4a33-db6d-d71e3010659f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m107.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZbKgNfk6h4Fa"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0VNNonASh4Fa"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/content/drive/MyDrive/BSAN Projects/NLP_735_Team1/yelp_train.csv', encoding='utf-8')\n",
    "test_df = pd.read_csv('/content/drive/MyDrive/BSAN Projects/NLP_735_Team1/yelp_test.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CYDMeZ_ZbJC8",
    "outputId": "c1b7daca-a3f5-4a9f-b440-5943d15f4039"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Sentiment', 'text'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7iG_QdCAh4Fc"
   },
   "outputs": [],
   "source": [
    "test = copy.deepcopy(test_df)\n",
    "train = copy.deepcopy(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCwSSudTh4Ff"
   },
   "source": [
    "# text pre process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gXqdy0iQh4Ff",
    "outputId": "d4597b0a-c0b8-489c-ad26-93f433ef2b8e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sYAmR2eEh4Ff",
    "outputId": "88af2900-f15e-4ec8-e426-d6ad562bfbfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting contractions\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting anyascii\n",
      "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyahocorasick\n",
      "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n"
     ]
    }
   ],
   "source": [
    "pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XxtoWdifh4Ff",
    "outputId": "959343c1-2173-44a5-abac-47c831aec357"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample sentence not ignored\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "import unicodedata\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def text_preprocess(text):\n",
    "    # Convert input to string\n",
    "    text = str(text)\n",
    "    \n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # Remove punctuation and stress marks using Unicode\n",
    "    text = ''.join(c for c in text if not unicodedata.category(c).startswith('P'))\n",
    "    text = ''.join(c for c in text if not unicodedata.category(c).startswith('Sk'))\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Lemmatize words using NLTK\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Remove stopwords, excluding negation words\n",
    "    negation_words = {'not', 'no', 'n\\'t', 'never', 'none', 'nobody', 'nothing', 'neither', 'nowhere', 'cant', 'can\\'t', 'dont', 'don\\'t'}\n",
    "    stop_words = set(stopwords.words('english')) - negation_words\n",
    "    filtered_tokens = [token for token in lemmatized_tokens if isinstance(token, str) and token.lower() not in stop_words]\n",
    "\n",
    "    # Reconstruct the processed text\n",
    "    processed_text = ' '.join(filtered_tokens)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "# Example usage\n",
    "input_text = \"This is a sample sentence, which won't be ignored.\"\n",
    "output_text = text_preprocess(input_text)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "wnJ4K81qh4Fg"
   },
   "outputs": [],
   "source": [
    "train['preprocessed_text'] = train['text'].apply(text_preprocess) \n",
    "test['preprocessed_text'] = test['text'].apply(text_preprocess) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KOkfCMSmeW51"
   },
   "outputs": [],
   "source": [
    "yelp = pd.concat([train,test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5SV6sWwxh4Fg",
    "outputId": "2d165b31-1e9b-4e33-bb1d-81bd3ece981c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp['Sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "GR31TiwQxy0f",
    "outputId": "e1d6e8ed-b79e-48b2-d5cc-dcc883326bcd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-347c8d1a-2dad-4ec0-8149-0fdb225b5450\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Unfortunately, the frustration of being Dr. Go...</td>\n",
       "      <td>Unfortunately frustration Dr Goldbergs patient...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n",
       "      <td>going Dr Goldberg 10 year think wa one 1st pat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I don't know what Dr. Goldberg was like before...</td>\n",
       "      <td>not know Dr Goldberg wa like moving Arizona le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm writing this review to give you a heads up...</td>\n",
       "      <td>writing review give head see Doctor office sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>All the food is great here. But the best thing...</td>\n",
       "      <td>food great best thing wing wing simply fantast...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-347c8d1a-2dad-4ec0-8149-0fdb225b5450')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-347c8d1a-2dad-4ec0-8149-0fdb225b5450 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-347c8d1a-2dad-4ec0-8149-0fdb225b5450');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   Sentiment                                               text  \\\n",
       "0          1  Unfortunately, the frustration of being Dr. Go...   \n",
       "1          2  Been going to Dr. Goldberg for over 10 years. ...   \n",
       "2          1  I don't know what Dr. Goldberg was like before...   \n",
       "3          1  I'm writing this review to give you a heads up...   \n",
       "4          2  All the food is great here. But the best thing...   \n",
       "\n",
       "                                   preprocessed_text  \n",
       "0  Unfortunately frustration Dr Goldbergs patient...  \n",
       "1  going Dr Goldberg 10 year think wa one 1st pat...  \n",
       "2  not know Dr Goldberg wa like moving Arizona le...  \n",
       "3  writing review give head see Doctor office sta...  \n",
       "4  food great best thing wing wing simply fantast...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdYta-iIh4Fg"
   },
   "source": [
    "# Nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BWW7otNmewc-",
    "outputId": "e0c3af58-bf2c-45e5-de27-38d88e8c777a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer fold 1\n",
      "Inner fold 1\n",
      "Inner fold 1, C=10, F1 Score: 0.9338389870904173\n",
      "Inner fold 2\n",
      "Inner fold 2, C=10, F1 Score: 0.9347825705833498\n",
      "Inner fold 3\n",
      "Inner fold 3, C=10, F1 Score: 0.9337314341312972\n",
      "Inner fold 4\n",
      "Inner fold 4, C=10, F1 Score: 0.9347705729057476\n",
      "Inner fold 5\n",
      "Inner fold 5, C=10, F1 Score: 0.9345675238187653\n",
      "Best inner params for outer fold 1: {'clf__C': 10}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.94      0.94      0.94     17940\n",
      "           2       0.94      0.94      0.94     17940\n",
      "\n",
      "    accuracy                           0.94     35880\n",
      "   macro avg       0.94      0.94      0.94     35880\n",
      "weighted avg       0.94      0.94      0.94     35880\n",
      "\n",
      "Outer fold 2\n",
      "Inner fold 1\n",
      "Inner fold 1, C=10, F1 Score: 0.9338389870904173\n",
      "Inner fold 2\n",
      "Inner fold 2, C=10, F1 Score: 0.9347825705833498\n",
      "Inner fold 3\n",
      "Inner fold 3, C=10, F1 Score: 0.9337314341312972\n",
      "Inner fold 4\n",
      "Inner fold 4, C=10, F1 Score: 0.9347705729057476\n",
      "Inner fold 5\n",
      "Inner fold 5, C=10, F1 Score: 0.9345675238187653\n",
      "Best inner params for outer fold 2: {'clf__C': 10}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      0.94      0.94     17940\n",
      "           2       0.94      0.93      0.94     17940\n",
      "\n",
      "    accuracy                           0.94     35880\n",
      "   macro avg       0.94      0.94      0.94     35880\n",
      "weighted avg       0.94      0.94      0.94     35880\n",
      "\n",
      "Outer fold 3\n",
      "Inner fold 1\n",
      "Inner fold 1, C=10, F1 Score: 0.9338389870904173\n",
      "Inner fold 2\n",
      "Inner fold 2, C=10, F1 Score: 0.9347825705833498\n",
      "Inner fold 3\n",
      "Inner fold 3, C=10, F1 Score: 0.9337314341312972\n",
      "Inner fold 4\n",
      "Inner fold 4, C=10, F1 Score: 0.9347705729057476\n",
      "Inner fold 5\n",
      "Inner fold 5, C=10, F1 Score: 0.9345675238187653\n",
      "Best inner params for outer fold 3: {'clf__C': 10}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.94      0.93      0.94     17940\n",
      "           2       0.93      0.94      0.94     17940\n",
      "\n",
      "    accuracy                           0.94     35880\n",
      "   macro avg       0.94      0.94      0.94     35880\n",
      "weighted avg       0.94      0.94      0.94     35880\n",
      "\n",
      "Outer fold 4\n",
      "Inner fold 1\n",
      "Inner fold 1, C=10, F1 Score: 0.9338389870904173\n",
      "Inner fold 2\n",
      "Inner fold 2, C=10, F1 Score: 0.9347825705833498\n",
      "Inner fold 3\n",
      "Inner fold 3, C=10, F1 Score: 0.9337314341312972\n",
      "Inner fold 4\n",
      "Inner fold 4, C=10, F1 Score: 0.9347705729057476\n",
      "Inner fold 5\n",
      "Inner fold 5, C=10, F1 Score: 0.9345675238187653\n",
      "Best inner params for outer fold 4: {'clf__C': 10}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.94      0.94      0.94     17940\n",
      "           2       0.94      0.94      0.94     17940\n",
      "\n",
      "    accuracy                           0.94     35880\n",
      "   macro avg       0.94      0.94      0.94     35880\n",
      "weighted avg       0.94      0.94      0.94     35880\n",
      "\n",
      "Outer fold 5\n",
      "Inner fold 1\n",
      "Inner fold 1, C=10, F1 Score: 0.9338389870904173\n",
      "Inner fold 2\n",
      "Inner fold 2, C=10, F1 Score: 0.9347825705833498\n",
      "Inner fold 3\n",
      "Inner fold 3, C=10, F1 Score: 0.9337314341312972\n",
      "Inner fold 4\n",
      "Inner fold 4, C=10, F1 Score: 0.9347705729057476\n",
      "Inner fold 5\n",
      "Inner fold 5, C=10, F1 Score: 0.9345675238187653\n",
      "Best inner params for outer fold 5: {'clf__C': 10}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      0.94      0.93     17940\n",
      "           2       0.94      0.93      0.93     17940\n",
      "\n",
      "    accuracy                           0.93     35880\n",
      "   macro avg       0.93      0.93      0.93     35880\n",
      "weighted avg       0.93      0.93      0.93     35880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "def train_and_evaluate(X_train_all, y_train_all, X_test_all, y_test_all):\n",
    "    skf_outer = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    skf_inner = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "    outer_fold = 1\n",
    "\n",
    "    for train_index, test_index in skf_outer.split(X_test_all, y_test_all):\n",
    "        print('Outer fold', outer_fold)\n",
    "\n",
    "        X_test, y_test = np.array(X_test_all)[test_index], np.array(y_test_all)[test_index]\n",
    "\n",
    "        inner_fold = 1\n",
    "        best_inner_score = 0\n",
    "        best_inner_params = None\n",
    "\n",
    "        for sub_train_index, dev_index in skf_inner.split(X_train_all, y_train_all):\n",
    "            print('Inner fold', inner_fold)\n",
    "\n",
    "            X_train, X_dev = np.array(X_train_all)[sub_train_index], np.array(X_train_all)[dev_index]\n",
    "            y_train, y_dev = np.array(y_train_all)[sub_train_index], np.array(y_train_all)[dev_index]\n",
    "\n",
    "            clf = Pipeline([('vect', TfidfVectorizer(smooth_idf=True)),\n",
    "                            ('clf', LogisticRegression(penalty='l2', solver='liblinear', max_iter=1500, random_state=0))])\n",
    "\n",
    "            for C in [10]:  # Tune the 'C' hyperparameter\n",
    "                clf.set_params(clf__C=C)\n",
    "                clf.fit(X_train, y_train)\n",
    "                y_dev_pred = clf.predict(X_dev)\n",
    "\n",
    "                dev_f1_score = f1_score(y_dev, y_dev_pred, average='macro')\n",
    "                print(f\"Inner fold {inner_fold}, C={C}, F1 Score: {dev_f1_score}\")\n",
    "\n",
    "                if dev_f1_score > best_inner_score:\n",
    "                    best_inner_score = dev_f1_score\n",
    "                    best_inner_params = {'clf__C': C}\n",
    "\n",
    "            inner_fold += 1\n",
    "\n",
    "        print(f\"Best inner params for outer fold {outer_fold}: {best_inner_params}\")\n",
    "\n",
    "        # Train the model using the best inner params on the entire train set of the current outer fold\n",
    "        clf.set_params(**best_inner_params)\n",
    "        clf.fit(X_train_all, y_train_all)\n",
    "\n",
    "        y_test_pred = clf.predict(X_test)\n",
    "        print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "        outer_fold += 1\n",
    "\n",
    "# Load your dataset here\n",
    "X = yelp['text']\n",
    "y = yelp['Sentiment']\n",
    "\n",
    "# Split the dataset into train and test sets using train_test_split\n",
    "X_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Call the train_and_evaluate function with the input data\n",
    "train_and_evaluate(X_train_all, y_train_all, X_test_all, y_test_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df25_51Ih4Fh"
   },
   "source": [
    "# Trainable embed and Pre-trained GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QC_XkEpFwOTH",
    "outputId": "c9cfa9fb-ecb3-48ae-95d6-fa97657f2635"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 100)          25849700  \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               117248    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 258       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 2)                8         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,967,214\n",
      "Trainable params: 25,967,210\n",
      "Non-trainable params: 4\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "5980/5980 [==============================] - 140s 22ms/step - loss: 0.2090 - accuracy: 0.9180 - val_loss: 0.1526 - val_accuracy: 0.9422\n",
      "Epoch 2/10\n",
      "5980/5980 [==============================] - 56s 9ms/step - loss: 0.1192 - accuracy: 0.9568 - val_loss: 0.1540 - val_accuracy: 0.9413\n",
      "Epoch 3/10\n",
      "5980/5980 [==============================] - 51s 8ms/step - loss: 0.0800 - accuracy: 0.9729 - val_loss: 0.1735 - val_accuracy: 0.9417\n",
      "Epoch 4/10\n",
      "5980/5980 [==============================] - 50s 8ms/step - loss: 0.0562 - accuracy: 0.9822 - val_loss: 0.1906 - val_accuracy: 0.9403\n",
      "1869/1869 [==============================] - 7s 4ms/step - loss: 0.1861 - accuracy: 0.9415\n",
      "Test loss: 0.18608330190181732, Test accuracy: 0.9414882659912109\n",
      "3738/3738 [==============================] - 11s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.94     60015\n",
      "           1       0.95      0.93      0.94     59585\n",
      "\n",
      "    accuracy                           0.94    119600\n",
      "   macro avg       0.94      0.94      0.94    119600\n",
      "weighted avg       0.94      0.94      0.94    119600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, BatchNormalization, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming you have a dataset called imdb with 'preprocessed_text' and 'label' columns\n",
    "X = yelp['text']\n",
    "y = yelp['Sentiment']\n",
    "\n",
    "# Initialize the LabelBinarizer and fit it on the labels\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y)\n",
    "\n",
    "# Transform the labels into one-hot encoded format\n",
    "y_one_hot = lb.transform(y)\n",
    "\n",
    "# Tokenize the text data\n",
    "max_sequence_length = 100\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert y_train and y_test to one-hot encoding\n",
    "num_classes = 2  # Replace this with the number of unique labels in your dataset\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "# Update vocab_size for the model\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Create the Keras model with the same architecture you provided earlier\n",
    "embedding_dim = 100\n",
    "lstm_units = 128\n",
    "\n",
    "model_trainable = Sequential()\n",
    "model_trainable.add(Embedding(vocab_size, embedding_dim, input_length=max_sequence_length))\n",
    "model_trainable.add(LSTM(lstm_units))\n",
    "model_trainable.add(Dense(2))\n",
    "model_trainable.add(BatchNormalization())\n",
    "model_trainable.add(Activation('sigmoid'))\n",
    "\n",
    "# Compile the model with the specified optimizer settings\n",
    "initial_learning_rate = 0.001\n",
    "decay_steps = 1000\n",
    "decay_rate = 0.9\n",
    "\n",
    "lr_schedule = ExponentialDecay(initial_learning_rate, decay_steps, decay_rate)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "model_trainable.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model_trainable.summary()\n",
    "\n",
    "# Early stopping based on validation accuracy\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3)\n",
    "\n",
    "# Train the model\n",
    "history_trainable = model_trainable.fit(X_train, y_train_one_hot, epochs=10, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "test_loss, test_accuracy = model_trainable.evaluate(X_test, y_test_one_hot, batch_size=64)\n",
    "print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy}')\n",
    "\n",
    "# Get predicted probabilities for the test set\n",
    "y_pred_probs = model_trainable.predict(X_test)\n",
    "\n",
    "# Convert probabilities to predicted classes\n",
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Convert one-hot encoded y_test back to class labels\n",
    "y_test_classes = np.argmax(y_test_one_hot, axis=1)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_classes, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0p0Uq4Wh4Fi"
   },
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnhKhn8Ml7lc"
   },
   "source": [
    "# Load GloVe embeddings and create embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EzBs9bJyOoeW",
    "outputId": "6ae1ca91-22f0-442b-d2e6-165d4202bf58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 100, 100)          25849700  \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               117248    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 2)                8         \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,967,214\n",
      "Trainable params: 117,510\n",
      "Non-trainable params: 25,849,704\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "5980/5980 [==============================] - 44s 7ms/step - loss: 0.2687 - accuracy: 0.8863 - val_loss: 0.2020 - val_accuracy: 0.9186\n",
      "Epoch 2/10\n",
      "5980/5980 [==============================] - 42s 7ms/step - loss: 0.1931 - accuracy: 0.9221 - val_loss: 0.1776 - val_accuracy: 0.9294\n",
      "Epoch 3/10\n",
      "5980/5980 [==============================] - 41s 7ms/step - loss: 0.1700 - accuracy: 0.9324 - val_loss: 0.1686 - val_accuracy: 0.9324\n",
      "Epoch 4/10\n",
      "5980/5980 [==============================] - 42s 7ms/step - loss: 0.1579 - accuracy: 0.9384 - val_loss: 0.1640 - val_accuracy: 0.9348\n",
      "Epoch 5/10\n",
      "5980/5980 [==============================] - 41s 7ms/step - loss: 0.1506 - accuracy: 0.9414 - val_loss: 0.1646 - val_accuracy: 0.9348\n",
      "Epoch 6/10\n",
      "5980/5980 [==============================] - 41s 7ms/step - loss: 0.1464 - accuracy: 0.9436 - val_loss: 0.1595 - val_accuracy: 0.9374\n",
      "Epoch 7/10\n",
      "5980/5980 [==============================] - 41s 7ms/step - loss: 0.1439 - accuracy: 0.9446 - val_loss: 0.1598 - val_accuracy: 0.9372\n",
      "Epoch 8/10\n",
      "5980/5980 [==============================] - 41s 7ms/step - loss: 0.1436 - accuracy: 0.9446 - val_loss: 0.1598 - val_accuracy: 0.9374\n",
      "Epoch 9/10\n",
      "5980/5980 [==============================] - 41s 7ms/step - loss: 0.1419 - accuracy: 0.9451 - val_loss: 0.1598 - val_accuracy: 0.9374\n",
      "Epoch 10/10\n",
      "5980/5980 [==============================] - 41s 7ms/step - loss: 0.1419 - accuracy: 0.9456 - val_loss: 0.1596 - val_accuracy: 0.9374\n",
      "1869/1869 [==============================] - 7s 4ms/step - loss: 0.1603 - accuracy: 0.9373\n",
      "Test loss: 0.16030627489089966, Test accuracy: 0.9372909665107727\n",
      "3738/3738 [==============================] - 10s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94     60015\n",
      "           1       0.94      0.93      0.94     59585\n",
      "\n",
      "    accuracy                           0.94    119600\n",
      "   macro avg       0.94      0.94      0.94    119600\n",
      "weighted avg       0.94      0.94      0.94    119600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, BatchNormalization, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Assuming you have a dataset called imdb with 'preprocessed_text' and 'label' columns\n",
    "X = yelp['text']\n",
    "y = yelp['Sentiment']\n",
    "\n",
    "# Initialize the LabelBinarizer and fit it on the labels\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y)\n",
    "\n",
    "# Transform the labels into one-hot encoded format\n",
    "y_one_hot = lb.transform(y)\n",
    "\n",
    "# Tokenize the text data\n",
    "max_sequence_length = 100\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert y_train and y_test to one-hot encoding\n",
    "num_classes = 2  # Replace this with the number of unique labels in your dataset\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "# Update vocab_size for the model\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove_embeddings = {}\n",
    "with open('/content/drive/MyDrive/BSAN Projects/glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.strip().split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        glove_embeddings[word] = vector\n",
    "\n",
    "# Create GloVe embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = glove_embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "\n",
    "# Create the Keras model with the same architecture you provided earlier\n",
    "embedding_dim = 100\n",
    "lstm_units = 128\n",
    "\n",
    "model_trainable = Sequential()\n",
    "model_trainable.add(Embedding(vocab_size, embedding_dim, input_length=max_sequence_length, weights=[embedding_matrix], trainable=False))\n",
    "model_trainable.add(LSTM(lstm_units))\n",
    "model_trainable.add(Dense(2))\n",
    "model_trainable.add(BatchNormalization())\n",
    "model_trainable.add(Activation('sigmoid'))\n",
    "\n",
    "# Compile the model with the specified optimizer settings\n",
    "initial_learning_rate = 0.001\n",
    "decay_steps = 1000\n",
    "decay_rate = 0.9\n",
    "\n",
    "lr_schedule = ExponentialDecay(initial_learning_rate, decay_steps, decay_rate)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "model_trainable.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model_trainable.summary()\n",
    "\n",
    "# Early stopping based on validation accuracy\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3)\n",
    "\n",
    "# Train the model\n",
    "history_trainable = model_trainable.fit(X_train, y_train_one_hot, epochs=10, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model_trainable.evaluate(X_test, y_test_one_hot, batch_size=64)\n",
    "print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy}')\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get predicted probabilities for the test set\n",
    "y_pred_probs = model_trainable.predict(X_test)\n",
    "\n",
    "# Convert probabilities to predicted classes\n",
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Convert one-hot encoded y_test back to class labels\n",
    "y_test_classes = np.argmax(y_test_one_hot, axis=1)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_classes, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jhch78uJh4Fj"
   },
   "source": [
    "# Trainable embed and Avg. Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h2cg6OHNPSrs",
    "outputId": "f1de58e1-82e7-4fea-c636-e678511028d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 100, 100)          25849700  \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 100, 128)          117248    \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 128)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 2)                8         \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,967,214\n",
      "Trainable params: 25,967,210\n",
      "Non-trainable params: 4\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "5980/5980 [==============================] - 114s 19ms/step - loss: 0.1979 - accuracy: 0.9221 - val_loss: 0.1640 - val_accuracy: 0.9378\n",
      "Epoch 2/10\n",
      "5980/5980 [==============================] - 54s 9ms/step - loss: 0.1164 - accuracy: 0.9569 - val_loss: 0.1460 - val_accuracy: 0.9445\n",
      "Epoch 3/10\n",
      "5980/5980 [==============================] - 52s 9ms/step - loss: 0.0788 - accuracy: 0.9722 - val_loss: 0.1620 - val_accuracy: 0.9434\n",
      "Epoch 4/10\n",
      "5980/5980 [==============================] - 53s 9ms/step - loss: 0.0553 - accuracy: 0.9816 - val_loss: 0.1839 - val_accuracy: 0.9412\n",
      "Epoch 5/10\n",
      "5980/5980 [==============================] - 52s 9ms/step - loss: 0.0416 - accuracy: 0.9869 - val_loss: 0.2077 - val_accuracy: 0.9396\n",
      "1869/1869 [==============================] - 7s 4ms/step - loss: 0.2050 - accuracy: 0.9396\n",
      "Test loss: 0.20501522719860077, Test accuracy: 0.9396237730979919\n",
      "3738/3738 [==============================] - 11s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94     60015\n",
      "           1       0.94      0.94      0.94     59585\n",
      "\n",
      "    accuracy                           0.94    119600\n",
      "   macro avg       0.94      0.94      0.94    119600\n",
      "weighted avg       0.94      0.94      0.94    119600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, BatchNormalization, Activation, GlobalAveragePooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming you have a dataset called imdb with 'preprocessed_text' and 'label' columns\n",
    "X = yelp['text']\n",
    "y = yelp['Sentiment']\n",
    "\n",
    "# Initialize the LabelBinarizer and fit it on the labels\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y)\n",
    "\n",
    "# Transform the labels into one-hot encoded format\n",
    "y_one_hot = lb.transform(y)\n",
    "\n",
    "# Tokenize the text data\n",
    "max_sequence_length = 100\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert y_train and y_test to one-hot encoding\n",
    "num_classes = 2  # Replace this with the number of unique labels in your dataset\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "# Update vocab_size for the model\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Create the Keras model with the same architecture you provided earlier\n",
    "embedding_dim = 100\n",
    "lstm_units = 128\n",
    "\n",
    "model_trainable = Sequential()\n",
    "model_trainable.add(Embedding(vocab_size, embedding_dim, input_length=max_sequence_length))\n",
    "model_trainable.add(LSTM(lstm_units, return_sequences=True))\n",
    "model_trainable.add(GlobalAveragePooling1D())\n",
    "model_trainable.add(Dense(2))\n",
    "model_trainable.add(BatchNormalization())\n",
    "model_trainable.add(Activation('sigmoid'))\n",
    "\n",
    "# Compile the model with the specified optimizer settings\n",
    "initial_learning_rate = 0.001\n",
    "decay_steps = 1000\n",
    "decay_rate = 0.9\n",
    "\n",
    "lr_schedule = ExponentialDecay(initial_learning_rate, decay_steps, decay_rate)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "model_trainable.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model_trainable.summary()\n",
    "\n",
    "# Early stopping based on validation accuracy\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3)\n",
    "\n",
    "# Train the model\n",
    "history_trainable = model_trainable.fit(X_train, y_train_one_hot, epochs=10, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "test_loss, test_accuracy = model_trainable.evaluate(X_test, y_test_one_hot, batch_size=64)\n",
    "print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy}')\n",
    "\n",
    "# Get predicted probabilities for the test set\n",
    "y_pred_probs = model_trainable.predict(X_test)\n",
    "\n",
    "# Convert probabilities to predicted classes\n",
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Convert one-hot encoded y_test back to class labels\n",
    "y_test_classes = np.argmax(y_test_one_hot, axis=1)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_classes, y_pred_classes))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
